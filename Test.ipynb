{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Part 1 Correlation of Mean Annual Temperature with Altitude in Bavaria\n",
    "In the first lectures we analysed the annual temperature in NRW by means of long time series. The observed temperature increase particularly in the last decade is most likely an indication of climate. We also observed that the station \"Kahler Asten\" shows systematic lower temperatures than other stations. We presumed this being an effect of decreasing temperature with topographic height, since \"Kahler Asten\" is among the highest points in NRW. \n",
    "\n",
    "Verify this hypothesis by means of data in Bavaria. This federal state reveals the broadest range of topographic heights, from 100m to more than 2800m above Normal-Null (NN). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 1\n",
    "Plot the annual mean temperatures of **years 2017, 2018, and 2019** versus altitude for the DWD stations in Bavaria. At first use the **altitudes from the station description file** `KL_Jahreswerte_Beschreibung_Stationen.txt` for the data set `/annual/kl/historical/`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing necessary libaries for Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime #used for time format conversion\n",
    "import os #access to host system to create directories and write files\n",
    "import ftplib #libary to access ftp server\n",
    "import codecs\n",
    "from zipfile import ZipFile #used for unzipping zip files\n",
    "import numpy as np #for replacing bad values with true NotaNumber from numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#making plots available in jupyter output line\n",
    "import pandas as pd #for pandas dataframe to read csv\n",
    "pd.options.display.max_seq_items = None #pandas printing options\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "#pd.set_option('display.width', 1000)\n",
    "from sklearn.linear_model import LinearRegression #linear regression to calculate a trendline from data points, Task 4"
   ]
  },
  {
   "source": [
    "## Defining variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_server = \"opendata.dwd.de\"\n",
    "ftp_user = \"anonymous\"\n",
    "ftp_passwd = \"\"\n",
    "ftp_dir =  \"/climate_environment/CDC/observations_germany/climate/annual/kl/historical/\"\n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "state = \"Bayern\"\n",
    "years = [2017, 2018, 2019]\n",
    "nyears = len(years)"
   ]
  },
  {
   "source": [
    "## Process functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_ftp(): #establishing connection to ftp server and check if it was successfull\n",
    "    global ftp\n",
    "    ftp = ftplib.FTP(host=ftp_server,user=ftp_user,passwd=ftp_passwd,timeout=None)\n",
    "    ftp.cwd(ftp_dir) #change ftp directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(): #create directories for datasets\n",
    "    global dir\n",
    "    dir = os.getcwd()\n",
    "    global local_dir\n",
    "    local_dir = dir+\"data/\"\n",
    "    os.makedirs(local_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df_from_ftp_dir():\n",
    "    lines = []\n",
    "    flist = []\n",
    "    try:    \n",
    "        res = ftp.retrlines(\"NLST\", lines.append)\n",
    "    except:\n",
    "        return\n",
    "    for line in lines:\n",
    "        fname = line\n",
    "        if (line.split(\"_\")[0] == 'jahreswerte'):\n",
    "            station_id = line.split(\"_\")[1]\n",
    "            fname = line #['KL_Jahreswerte_Beschreibung_Stationen.txt', 'jahreswerte_KL_00001_19310101_19851231_hist.zip',...]\n",
    "            flist.append([station_id, fname])\n",
    "        elif (line.split(\"_\")[0] == 'KL'):\n",
    "            global station_desc_fname\n",
    "            station_desc_fname = line\n",
    "        else:None;\n",
    "    df = pd.DataFrame(flist,columns=[\"station_id\",\"fname\"]) \n",
    "    #dates from file names differ from station description eg station_id 769 date_to 2005 in filename, 2020 in station_desc, so choosing dates from description and merging with filenames by station_id\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'FTP' object has no attribute 'open'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dd20dce9fe37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_desc_fname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_fwf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FTP' object has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "    #not working\n",
    "    with ftp.open(station_desc_fname) as f:\n",
    "        df1 = pd.read_fwf(f)\n",
    "        print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_desc_txt_to_df():\n",
    "    local_fullname = local_dir+station_desc_fname\n",
    "    localfile = open(local_fullname, 'wb')\n",
    "    ftp.retrbinary('RETR ' + station_desc_fname, localfile.write, 1024)\n",
    "    localfile.close()\n",
    "    ftp_dir + station_fname, local_ftp_station_dir + station_fname)\n",
    "\n",
    "\n",
    "    file = codecs.open(txtfile,\"r\",\"utf-8\")\n",
    "    r = file.readline()\n",
    "    file.close()\n",
    "    colnames_de = r.split()\n",
    "    translate =     {'Stations_id':'station_id',\n",
    "     'von_datum':'date_from',\n",
    "     'bis_datum':'date_to',\n",
    "     'Stationshoehe':'altitude',\n",
    "     'geoBreite': 'latitude',\n",
    "     'geoLaenge': 'longitude',\n",
    "     'Stationsname':'name',\n",
    "     'Bundesland':'state'}\n",
    "    colnames_en = [translate[h] for h in colnames_de]\n",
    "    df = pd.read_fwf(txtfile,skiprows=2,colspecs='infer',names=colnames_en, parse_dates=[\"date_from\",\"date_to\"],index_col = 0)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations():\n",
    "    global local_zip_list\n",
    "    local_zip_list = []\n",
    "    for station_id in station_ids_selected:\n",
    "        try:\n",
    "            fname = df_zips[\"name\"][station_id]\n",
    "            grabFile(ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "            local_zip_list.append(fname)\n",
    "        except:\n",
    "            (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_ts_to_df(fname): \n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM_BEGINN\", parse_dates = [\"MESS_DATUM_BEGINN\", \"MESS_DATUM_ENDE\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "    df = df[(df.index >= date_from) & (df.index <= date_to)]\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_merge():\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = kl_ts_to_df(myfile)\n",
    "                if len(dftmp) > 0:\n",
    "                    s = dftmp[\"ja_tt\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                    df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "                else:\n",
    "                    (\"\")\n",
    "    df = df.dropna(axis='columns')\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_append():\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0]\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = kl_ts_to_df(myfile)\n",
    "                if len(dftmp) > 0:\n",
    "                    dftmp = dftmp.merge(df_stations,how=\"inner\",left_on=\"stations_id\",right_on=\"station_id\",right_index=True)\n",
    "                    df = df.append(dftmp)\n",
    "                else:\n",
    "                    (\"\")\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    \n",
    "    df.replace(to_replace = -999,value = (np.nan),inplace=True)\n",
    "    \n",
    "    df = df.dropna(subset = [(str(o1)),(str(o2))])\n",
    "    \n",
    "    #ind1 = df[df[str(o1)]==-999].index\n",
    "    #df.drop(ind1,inplace=True)\n",
    "    #ind2 = df[df[str(o2)]==-999].index\n",
    "    #df.drop(ind2,inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    retranslate = {\"ja_tt\":\"Average Temperature\",\"ja_tx\":\"Yearly Average Max Temperature\",\"ja_tn\":\"Yearly Average Min Temperature\",\"ja_fk\":\"Average Windforce\",\"ja_sd_s\":\"Sum Yearly Sunshine Duration\",\"ja_mx_tx\":\"Absolute Max Temperature\",\"ja_mx_tn\":\"Absolute Min Temperature\",\"ja_rr\":\"Sum Yearly Precipitation\",\"ja_mx_rs\":\"Max Precipitation Height\",\"altitude\":\"Altitude\",\"latitude\":\"Latitude\",\"longitude\":\"Longitude\"}\n",
    "    po1 = retranslate[(o1)]\n",
    "    po2 = retranslate[(o2)]\n",
    "    fpo1 = po1.replace(\" \", \"_\")\n",
    "    fpo2 = po2.replace(\" \", \"_\")\n",
    "\n",
    "    df_plot = df_appended_ts\n",
    "    \n",
    "    df_corr = pd.DataFrame(df_appended_ts.loc[:,o2])\n",
    "    df_corr[o1] = df_appended_ts.loc[:,o1]\n",
    "    Y = df_appended_ts.loc[:,o1].values.reshape(-1, 1)\n",
    "    X = df_appended_ts.loc[:,o2].values.reshape(-1, 1)\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X, Y)\n",
    "    score = linear_regressor.score(X, Y)\n",
    "    Y_pred = linear_regressor.predict(X)\n",
    "\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(dpi=136, figsize=(8,6))\n",
    "    b = round((linear_regressor.intercept_[0]),4)\n",
    "    m = round((linear_regressor.coef_[0][0]),4)\n",
    "    sx = 0.35 * ax1.get_xlim()[1]\n",
    "    sy = 1.69 * ax1.get_ylim()[0]\n",
    "    r = round(score,4)\n",
    "    ax1.plot(X, Y_pred, color='red')\n",
    "    ax1.plot(df_plot[o2],df_plot[o1],\".\")\n",
    "    ax1.set_ylabel(po1)\n",
    "    ax1.set_xlabel(po2)\n",
    "    ax1.set_title(po1+\" vs. \"+po2+\" in Year \" + year_selected + \" at DWD Stations in \" + state+\"\\ny=\"+str(m)+\"*x+\"+str(b)+\", R^2= \"+str(r))\n",
    "\n",
    "    #ax1.text(x=sx,y=sy,s=(\"y=\"+str(m)+\"*x + \"+str(b)+\", R^2= \"+str(r)))\n",
    "\n",
    "    ax1.grid(True)\n",
    "    plt.show()\n",
    "    fig1.savefig(fpo1+\"_\"+fpo2+\"_\"+year_selected+\"_DWD_Stations_\"+state+\".png\")\n",
    "    print(\"A low R^2 value indicates, that the regression model is not fitting well (no strong correlation of data points).\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process():\n",
    "    create_dir()\n",
    "    connect_ftp()\n",
    "    global df_zips\n",
    "    df_zips = gen_df_from_ftp_dir()\n",
    "    df_zips.set_index(\"station_id\", inplace = True)\n",
    "    \n",
    "    station_grab()\n",
    "    global basename = os.path.splitext(station_fname)[0]\n",
    "    global df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "    global station_ids_selected = df_stations[df_stations['state'].str.contains(state)].index\n",
    "    download_stations()\n",
    "    global df_merged_ts\n",
    "    df_merged_ts = ts_merge()\n",
    "    df_merged_ts.to_csv(local_ts_merged_dir + \"ts_merged.csv\",sep=\";\")\n",
    "    global df_appended_ts = ts_append()\n",
    "    df_appended_ts.to_csv(local_ts_appended_dir + \"ts_appended.csv\",sep=\";\")"
   ]
  },
  {
   "source": [
    "## Main run function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading...\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'process' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5cde9956cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Plotting...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading...\\n\")\n",
    "process()\n",
    "print(\"Plotting...\\n\")\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir()\n",
    "connect_ftp()\n",
    "global df_zips\n",
    "df_zips = gen_df_from_ftp_dir()\n",
    "df_zips.set_index(\"station_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}