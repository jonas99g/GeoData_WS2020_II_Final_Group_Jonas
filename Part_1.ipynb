{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Part 1 Correlation of Mean Annual Temperature with Altitude in Bavaria\n",
    "In the first lectures we analysed the annual temperature in NRW by means of long time series. The observed temperature increase particularly in the last decade is most likely an indication of climate. We also observed that the station \"Kahler Asten\" shows systematic lower temperatures than other stations. We presumed this being an effect of decreasing temperature with topographic height, since \"Kahler Asten\" is among the highest points in NRW. \n",
    "\n",
    "Verify this hypothesis by means of data in Bavaria. This federal state reveals the broadest range of topographic heights, from 100m to more than 2800m above Normal-Null (NN). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 1\n",
    "Plot the annual mean temperatures of **years 2017, 2018, and 2019** versus altitude for the DWD stations in Bavaria. At first use the **altitudes from the station description file** `KL_Jahreswerte_Beschreibung_Stationen.txt` for the data set `/annual/kl/historical/`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing necessary libaries for Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # used for time format conversion\n",
    "import os # access to host system to create directories and write files\n",
    "import ftplib # libary to access ftp server\n",
    "import urllib3 \n",
    "import codecs\n",
    "from zipfile import ZipFile # used for unzipping zip files\n",
    "import numpy as np # numpy arrays and functions for example replacing bad values with true NotaNumber\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# making plots available in jupyter output line\n",
    "import pandas as pd # for pandas dataframe to read csv\n",
    "pd.options.display.max_seq_items = None # pandas printing options\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # linear regression to calculate a trendline from data points, Task 4"
   ]
  },
  {
   "source": [
    "## Defining variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_server = \"opendata.dwd.de\" # root of file server\n",
    "ftp_user = \"anonymous\"\n",
    "ftp_passwd = \"\"\n",
    "ftp_dir =  \"/climate_environment/CDC/observations_germany/climate/annual/kl/historical/\" # directory\n",
    "state = \"Bayern\" # Selected state to filter\n",
    "years = [2017, 2018, 2019] # selected years\n",
    "nyears = len(years) # length of years list\n",
    "year_from = datetime.strptime(str(years[0])+\"0101\", '%Y%m%d') # lowest year from the list\n",
    "year_to = datetime.strptime(str(years[nyears-1])+\"1231\", '%Y%m%d') # highest year from the list\n",
    "stations_fname = \"\" # initializing variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_ftp(): # establishing connection to ftp server and check if it was successfull\n",
    "    ftp = ftplib.FTP(ftp_server) # creating ftp server instance\n",
    "    res = ftp.login(user = ftp_user, passwd = ftp_passwd) # logging in to server\n",
    "    ret = ftp.cwd(ftp_dir) # Changing into correct ftp directory\n",
    "    return ftp # return configured and connected ftp instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df_ftp_dir():\n",
    "    lines = [] # buffer for storing lines of ftp directory\n",
    "    flist = [] # buffer for temporarily storing station_idm, zip file names and product file name\n",
    "    try:\n",
    "        res = ftp.retrlines(\"NLST\", lines.append) # retrieve lines with NLST ftp command, which lsits file names including extention, the returned lines are appended to the lines buffer\n",
    "    except:\n",
    "        return\n",
    "    global stations_fname # setting global variable to use filename later\n",
    "    stations_fname = lines[0] # storing first line, which is the file name of the station description\n",
    "    lines.pop(0) # removing station description file from buffer to read only zip files later\n",
    "    for line in lines: # looping through elements of the lines buffer\n",
    "        pname = \"produkt_klima_jahr_\"+line.split(\"_\")[3]+\"_\"+line.split(\"_\")[4]+\"_\"+line.split(\"_\")[2]+\".txt\" # generating product file name\n",
    "        flist.append([int(line.split(\"_\")[2]), line, pname]) # reading variables into temporary list\n",
    "    df_ftp_dir = pd.DataFrame(flist,columns=[\"station_id\", \"fname\", \"pname\"]) # creating a pandas dataframe from flist, defining column names for elements in the list\n",
    "    df_ftp_dir.set_index(\"station_id\", inplace = True) # setting station_id column as index and replacing the standard numeration\n",
    "    return df_ftp_dir # return the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df_station_desc():\n",
    "    try:\n",
    "        ftp.retrbinary('RETR '+ stations_fname, open(stations_fname, 'wb').write) # retrieve the binary code from the stations_fname file from ftp and writing to a newly opened file with the same filename\n",
    "    except:\n",
    "        return\n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d') for d in dates] # function for parsing the dates from the txt, for each column in a row the value is converted to a string and parsed into a datetime object\n",
    "    df_station_desc = pd.read_fwf(stations_fname, skiprows = 2, header=None, parse_dates = [1,2], date_parser = dateparse, encoding = 'latin-1') # encoding of txt is ISO-8859-1/latin (german umlaute)\n",
    "    # Read the table of fixed-width formatted lines from stations_fname file into DataFrame, skipping 2 rows, do not set a header, so that indeces are used, the columns 1 \"von_datum\" and 2 \"bis_datum\" are parsed as dates with the function dateparse\n",
    "    df_station_desc.columns = [\"station_id\", \"date_from\", \"date_to\", \"altitude\", \"latitude\", \"longitude\",\"name\", \"state\"] # english column names are set\n",
    "    df_station_desc.set_index(\"station_id\", inplace = True) # setting station_id column as index and replacing the standard numeration\n",
    "    df_station_desc_query = df_station_desc.query('state == @state & date_from <= @year_from & date_to >= @year_to')\n",
    "    return df_station_desc_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_stations(st_id):#, year):\n",
    "\n",
    "    f_name = df_ftp_dir[\"station_id\" == st_id][\"fname\"] # id from function, selecting corresponding row from ftp directory dataframe and the \"fname\" column and returning the value\n",
    "    p_name = df_ftp_dir[\"station_id\" == st_id][\"pname\"] # id from function, selecting corresponding row from ftp directory dataframe and the \"pname\" column and returning the value\n",
    "    try:\n",
    "        ftp.retrbinary('RETR ' + f_name, open( fname, 'wb', encoding='utf-8',\n",
    "                 errors='ignore').write) # retrieve the binary code from the fname zip file from ftp and writing to a newly opened file with the same filename\n",
    "    except:\n",
    "        return\n",
    "    with ZipFile(f_name) as myzip: # recently downloaded file is initialized as ZipFile\n",
    "        with myzip.open(p_name) as myfile: # zip file is opened and the containing product file is opened as myfile \n",
    "            df = pd.read_csv(myfile, sep = ';', encoding = 'utf-8')\n",
    "            print(df)\n",
    "        \n",
    "        '''\n",
    "        with ZipFile(fname, 'r') as zipObj:\n",
    "            zipObj.extract(pname, 'temp_csv')\n",
    "        \n",
    "        fname.unzip(pname)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "def kl_ts_to_df(fname): \n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM_BEGINN\", parse_dates = [\"MESS_DATUM_BEGINN\", \"MESS_DATUM_ENDE\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "    df = df[(df.index >= date_from) & (df.index <= date_to)]\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def ts_merge():\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = kl_ts_to_df(myfile)\n",
    "                if len(dftmp) > 0:\n",
    "                    s = dftmp[\"ja_tt\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                    df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "                else:\n",
    "                    (\"\")\n",
    "    df = df.dropna(axis='columns')\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def ts_append():\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0]\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = kl_ts_to_df(myfile)\n",
    "                if len(dftmp) > 0:\n",
    "                    dftmp = dftmp.merge(df_stations,how=\"inner\",left_on=\"stations_id\",right_on=\"station_id\",right_index=True)\n",
    "                    df = df.append(dftmp)\n",
    "                else:\n",
    "                    (\"\")\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    \n",
    "    df.replace(to_replace = -999,value = (np.nan),inplace=True)\n",
    "    \n",
    "    df = df.dropna(subset = [(str(o1)),(str(o2))])\n",
    "    \n",
    "    #ind1 = df[df[str(o1)]==-999].index\n",
    "    #df.drop(ind1,inplace=True)\n",
    "    #ind2 = df[df[str(o2)]==-999].index\n",
    "    #df.drop(ind2,inplace=True)\n",
    "    return(df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def plot():\n",
    "    retranslate = {\"ja_tt\":\"Average Temperature\",\"ja_tx\":\"Yearly Average Max Temperature\",\"ja_tn\":\"Yearly Average Min Temperature\",\"ja_fk\":\"Average Windforce\",\"ja_sd_s\":\"Sum Yearly Sunshine Duration\",\"ja_mx_tx\":\"Absolute Max Temperature\",\"ja_mx_tn\":\"Absolute Min Temperature\",\"ja_rr\":\"Sum Yearly Precipitation\",\"ja_mx_rs\":\"Max Precipitation Height\",\"altitude\":\"Altitude\",\"latitude\":\"Latitude\",\"longitude\":\"Longitude\"}\n",
    "    po1 = retranslate[(o1)]\n",
    "    po2 = retranslate[(o2)]\n",
    "    fpo1 = po1.replace(\" \", \"_\")\n",
    "    fpo2 = po2.replace(\" \", \"_\")\n",
    "\n",
    "    df_plot = df_appended_ts\n",
    "    \n",
    "    df_corr = pd.DataFrame(df_appended_ts.loc[:,o2])\n",
    "    df_corr[o1] = df_appended_ts.loc[:,o1]\n",
    "    Y = df_appended_ts.loc[:,o1].values.reshape(-1, 1)\n",
    "    X = df_appended_ts.loc[:,o2].values.reshape(-1, 1)\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X, Y)\n",
    "    score = linear_regressor.score(X, Y)\n",
    "    Y_pred = linear_regressor.predict(X)\n",
    "\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(dpi=136, figsize=(8,6))\n",
    "    b = round((linear_regressor.intercept_[0]),4)\n",
    "    m = round((linear_regressor.coef_[0][0]),4)\n",
    "    sx = 0.35 * ax1.get_xlim()[1]\n",
    "    sy = 1.69 * ax1.get_ylim()[0]\n",
    "    r = round(score,4)\n",
    "    ax1.plot(X, Y_pred, color='red')\n",
    "    ax1.plot(df_plot[o2],df_plot[o1],\".\")\n",
    "    ax1.set_ylabel(po1)\n",
    "    ax1.set_xlabel(po2)\n",
    "    ax1.set_title(po1+\" vs. \"+po2+\" in Year \" + year_selected + \" at DWD Stations in \" + state+\"\\ny=\"+str(m)+\"*x+\"+str(b)+\", R^2= \"+str(r))\n",
    "\n",
    "    #ax1.text(x=sx,y=sy,s=(\"y=\"+str(m)+\"*x + \"+str(b)+\", R^2= \"+str(r)))\n",
    "\n",
    "    ax1.grid(True)\n",
    "    plt.show()\n",
    "    fig1.savefig(fpo1+\"_\"+fpo2+\"_\"+year_selected+\"_DWD_Stations_\"+state+\".png\")\n",
    "    print(\"A low R^2 value indicates, that the regression model is not fitting well (no strong correlation of data points).\\n\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Main run function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def process():\n",
    "    ftp = connect_ftp()\n",
    "    df_ftp_dir = gen_df_ftp_dir()\n",
    "    df_station_desc = gen_df_station_desc()\n",
    "    station_ids_selected = []\n",
    "    grab_stations(station_ids_selected)\n",
    "\n",
    "    download_stations()\n",
    "    global df_merged_ts\n",
    "    df_merged_ts = ts_merge()\n",
    "    df_merged_ts.to_csv(local_ts_merged_dir + \"ts_merged.csv\",sep=\";\")\n",
    "    global df_appended_ts = ts_append()\n",
    "    df_appended_ts.to_csv(local_ts_appended_dir + \"ts_appended.csv\",sep=\";\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp = connect_ftp()\n",
    "df_ftp_dir = gen_df_ftp_dir()\n",
    "df_station_desc_query = gen_df_station_desc()\n",
    "#station_query = df_station_desc.query('state == @state & date_from <= @year_from & date_to >= @year_to')\n",
    "#for el in station_query[\"station_id\"]:\n",
    "    #print(el)\n",
    "#grab_stations(\"000003\")\n"
   ]
  },
  {
   "source": [
    "dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "year_1 = datetime.strptime(str(years[0])+\"1231\", '%Y%m%d')\n",
    "year_2 = datetime.strptime(str(years[1])+\"1231\", '%Y%m%d')\n",
    "year_3 = datetime.strptime(str(years[2])+\"1231\", '%Y%m%d')\n",
    "f_name = df_ftp_dir.loc[232,\"fname\"]\n",
    "p_name = df_ftp_dir.loc[232,\"pname\"]\n",
    "ftp.retrbinary('RETR ' + f_name, open( f_name, 'wb').write)\n",
    "with ZipFile(f_name) as myzip:\n",
    "    with myzip.open(p_name) as myfile:\n",
    "        df = pd.read_csv(myfile, delimiter=\";\", encoding=\"utf8\", parse_dates = [\"MESS_DATUM_BEGINN\", \"MESS_DATUM_ENDE\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "        df_q = df.loc[(df['MESS_DATUM_ENDE'] == year_1) | (df['MESS_DATUM_ENDE'] == year_2) | (df['MESS_DATUM_ENDE'] == year_3),[\"STATIONS_ID\", \"MESS_DATUM_ENDE\", \"JA_TT\"]]\n",
    "        #print(df_q[[\"STATIONS_ID\", \"JA_TT\", \"MESS_DATUM_ENDE\"]])\n",
    "        #print(df_q)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_stations(stations, year):\n",
    "    df = pd.DataFrame\n",
    "    lst = []\n",
    "    dateparse = lambda dates: [pd.to_datetime(d, yearfirst = True) for d in dates]\n",
    "    date = pd.to_datetime(str(year)+\"1231\", yearfirst = True)\n",
    "    for st_id in stations:\n",
    "        f_name = df_ftp_dir.loc[st_id,\"fname\"]\n",
    "        p_name = df_ftp_dir.loc[st_id,\"pname\"]\n",
    "        ftp.retrbinary('RETR ' + f_name, open( f_name, 'wb').write)\n",
    "        with ZipFile(f_name) as myzip:\n",
    "            with myzip.open(p_name) as myfile:\n",
    "                df_f = pd.read_csv(myfile, delimiter=\";\", encoding=\"utf8\", parse_dates = [\"MESS_DATUM_BEGINN\", \"MESS_DATUM_ENDE\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "                df_q = df_f.query('MESS_DATUM_ENDE == @date')\n",
    "                #df_q = df_f.loc[(df_f['MESS_DATUM_ENDE'] == date),[\"STATIONS_ID\", \"MESS_DATUM_ENDE\", \"JA_TT\"]]\n",
    "                lst = df_q[\"STATIONS_ID\"].values, df_q[\"MESS_DATUM_ENDE\"].values, df_q[\"JA_TT\"].values\n",
    "                #print(df_q[\"STATIONS_ID\", \"MESS_DATUM_ENDE\", \"JA_TT\"])\n",
    "                #print(df_q)\n",
    "                print(lst)\n",
    "                #df = df.append(df_q)\n",
    "    #df.index(\"STATIONS_ID\", inplace = True)\n",
    "    #df.columns(\"\")\n",
    "    #return df\n",
    "    #return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([232]), array(['2017-12-31T00:00:00.000000000'], dtype='datetime64[ns]'), array([9.15]))\n"
     ]
    }
   ],
   "source": [
    "grab_stations([232], 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                      fname  \\\n",
       "station_id                                                    \n",
       "1           jahreswerte_KL_00001_19310101_19851231_hist.zip   \n",
       "3           jahreswerte_KL_00003_18510101_20101231_hist.zip   \n",
       "44          jahreswerte_KL_00044_19720101_20191231_hist.zip   \n",
       "52          jahreswerte_KL_00052_19730101_20011231_hist.zip   \n",
       "61          jahreswerte_KL_00061_19760101_19771231_hist.zip   \n",
       "...                                                     ...   \n",
       "15963       jahreswerte_KL_15963_19530101_20031231_hist.zip   \n",
       "15965       jahreswerte_KL_15965_19700101_19831231_hist.zip   \n",
       "15979       jahreswerte_KL_15979_19480101_19781231_hist.zip   \n",
       "16085       jahreswerte_KL_16085_19610101_19611231_hist.zip   \n",
       "19087       jahreswerte_KL_19087_19580101_19941231_hist.zip   \n",
       "\n",
       "                                                     pname  \n",
       "station_id                                                  \n",
       "1           produkt_klima_jahr_19310101_19851231_00001.txt  \n",
       "3           produkt_klima_jahr_18510101_20101231_00003.txt  \n",
       "44          produkt_klima_jahr_19720101_20191231_00044.txt  \n",
       "52          produkt_klima_jahr_19730101_20011231_00052.txt  \n",
       "61          produkt_klima_jahr_19760101_19771231_00061.txt  \n",
       "...                                                    ...  \n",
       "15963       produkt_klima_jahr_19530101_20031231_15963.txt  \n",
       "15965       produkt_klima_jahr_19700101_19831231_15965.txt  \n",
       "15979       produkt_klima_jahr_19480101_19781231_15979.txt  \n",
       "16085       produkt_klima_jahr_19610101_19611231_16085.txt  \n",
       "19087       produkt_klima_jahr_19580101_19941231_19087.txt  \n",
       "\n",
       "[1076 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>pname</th>\n    </tr>\n    <tr>\n      <th>station_id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>jahreswerte_KL_00001_19310101_19851231_hist.zip</td>\n      <td>produkt_klima_jahr_19310101_19851231_00001.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jahreswerte_KL_00003_18510101_20101231_hist.zip</td>\n      <td>produkt_klima_jahr_18510101_20101231_00003.txt</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>jahreswerte_KL_00044_19720101_20191231_hist.zip</td>\n      <td>produkt_klima_jahr_19720101_20191231_00044.txt</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>jahreswerte_KL_00052_19730101_20011231_hist.zip</td>\n      <td>produkt_klima_jahr_19730101_20011231_00052.txt</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>jahreswerte_KL_00061_19760101_19771231_hist.zip</td>\n      <td>produkt_klima_jahr_19760101_19771231_00061.txt</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15963</th>\n      <td>jahreswerte_KL_15963_19530101_20031231_hist.zip</td>\n      <td>produkt_klima_jahr_19530101_20031231_15963.txt</td>\n    </tr>\n    <tr>\n      <th>15965</th>\n      <td>jahreswerte_KL_15965_19700101_19831231_hist.zip</td>\n      <td>produkt_klima_jahr_19700101_19831231_15965.txt</td>\n    </tr>\n    <tr>\n      <th>15979</th>\n      <td>jahreswerte_KL_15979_19480101_19781231_hist.zip</td>\n      <td>produkt_klima_jahr_19480101_19781231_15979.txt</td>\n    </tr>\n    <tr>\n      <th>16085</th>\n      <td>jahreswerte_KL_16085_19610101_19611231_hist.zip</td>\n      <td>produkt_klima_jahr_19610101_19611231_16085.txt</td>\n    </tr>\n    <tr>\n      <th>19087</th>\n      <td>jahreswerte_KL_19087_19580101_19941231_hist.zip</td>\n      <td>produkt_klima_jahr_19580101_19941231_19087.txt</td>\n    </tr>\n  </tbody>\n</table>\n<p>1076 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "df_ftp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "73         1953-01-01 2020-12-31       340   48.6159    13.0506   \n",
       "142        1955-01-01 2020-12-31       511   48.4060    11.3117   \n",
       "151        1881-01-01 2020-12-31       382   49.4691    11.8546   \n",
       "154        1994-01-01 2020-12-31       516   48.0197    12.2925   \n",
       "191        1884-01-01 2020-12-31       217   49.9694     9.9114   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "7412       2006-10-01 2020-12-31       340   50.0083     9.4238   \n",
       "7424       2007-01-01 2020-12-31       457   47.7724    12.9073   \n",
       "7431       2008-01-01 2020-12-31       604   48.0130    11.5524   \n",
       "13710      2009-01-01 2020-12-31       490   48.5734    12.2576   \n",
       "15555      2017-01-01 2020-12-31       815   47.8761    10.5849   \n",
       "\n",
       "                               name   state  \n",
       "station_id                                   \n",
       "73             Aldersbach-Kriestorf  Bayern  \n",
       "142           Altomünster-Maisbrunn  Bayern  \n",
       "151         Amberg-Unterammersricht  Bayern  \n",
       "154                Amerang-Pfaffing  Bayern  \n",
       "191              Arnstein-Müdesheim  Bayern  \n",
       "...                             ...     ...  \n",
       "7412             Neuhütten/Spessart  Bayern  \n",
       "7424                         Piding  Bayern  \n",
       "7431           Oberhaching-Laufzorn  Bayern  \n",
       "13710              Landshut-Reithof  Bayern  \n",
       "15555         Kaufbeuren-Oberbeuren  Bayern  \n",
       "\n",
       "[102 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_from</th>\n      <th>date_to</th>\n      <th>altitude</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>name</th>\n      <th>state</th>\n    </tr>\n    <tr>\n      <th>station_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>73</th>\n      <td>1953-01-01</td>\n      <td>2020-12-31</td>\n      <td>340</td>\n      <td>48.6159</td>\n      <td>13.0506</td>\n      <td>Aldersbach-Kriestorf</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>1955-01-01</td>\n      <td>2020-12-31</td>\n      <td>511</td>\n      <td>48.4060</td>\n      <td>11.3117</td>\n      <td>Altomünster-Maisbrunn</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>1881-01-01</td>\n      <td>2020-12-31</td>\n      <td>382</td>\n      <td>49.4691</td>\n      <td>11.8546</td>\n      <td>Amberg-Unterammersricht</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>1994-01-01</td>\n      <td>2020-12-31</td>\n      <td>516</td>\n      <td>48.0197</td>\n      <td>12.2925</td>\n      <td>Amerang-Pfaffing</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>1884-01-01</td>\n      <td>2020-12-31</td>\n      <td>217</td>\n      <td>49.9694</td>\n      <td>9.9114</td>\n      <td>Arnstein-Müdesheim</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7412</th>\n      <td>2006-10-01</td>\n      <td>2020-12-31</td>\n      <td>340</td>\n      <td>50.0083</td>\n      <td>9.4238</td>\n      <td>Neuhütten/Spessart</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>7424</th>\n      <td>2007-01-01</td>\n      <td>2020-12-31</td>\n      <td>457</td>\n      <td>47.7724</td>\n      <td>12.9073</td>\n      <td>Piding</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>7431</th>\n      <td>2008-01-01</td>\n      <td>2020-12-31</td>\n      <td>604</td>\n      <td>48.0130</td>\n      <td>11.5524</td>\n      <td>Oberhaching-Laufzorn</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>13710</th>\n      <td>2009-01-01</td>\n      <td>2020-12-31</td>\n      <td>490</td>\n      <td>48.5734</td>\n      <td>12.2576</td>\n      <td>Landshut-Reithof</td>\n      <td>Bayern</td>\n    </tr>\n    <tr>\n      <th>15555</th>\n      <td>2017-01-01</td>\n      <td>2020-12-31</td>\n      <td>815</td>\n      <td>47.8761</td>\n      <td>10.5849</td>\n      <td>Kaufbeuren-Oberbeuren</td>\n      <td>Bayern</td>\n    </tr>\n  </tbody>\n</table>\n<p>102 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df_station_desc_query"
   ]
  }
 ]
}